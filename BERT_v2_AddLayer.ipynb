{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ming Xuan\\AppData\\Local\\Temp\\ipykernel_4080\\2238641776.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords \n",
    "from collections import Counter\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(files):\n",
    "\n",
    "    data_path = os.getcwd()+'/Data/'\n",
    "    combined_df = pd.DataFrame(columns=['text', 'label'])\n",
    "    df_columns = ['text', 'label']\n",
    "    \n",
    "    for x in files:\n",
    "\n",
    "        df = pd.read_csv(data_path+x)\n",
    "        df = df[df.columns[:2]]\n",
    "        df.columns = df_columns\n",
    "        df['text'] = df['text'].astype(str)\n",
    "\n",
    "        combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ming Xuan\\AppData\\Local\\Temp\\ipykernel_4080\\622898227.py:14: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_df = pd.concat([combined_df, df], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>oh my gosh</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trouble sleeping, confused mind, restless hear...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All wrong, back off dear, forward doubt. Stay ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I've shifted my focus to something else but I'...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm restless and restless, it's been a month n...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32511</th>\n",
       "      <td>\"Nothing beats the feeling of achieving someth...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32512</th>\n",
       "      <td>Too much homework Feeling Stressed!</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32513</th>\n",
       "      <td>overworked in school,,  causes me  headache</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32514</th>\n",
       "      <td>How come my friends immediately get to know st...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32515</th>\n",
       "      <td>toady I wake at 5am</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32516 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0                                             oh my gosh    1.0\n",
       "1      trouble sleeping, confused mind, restless hear...    1.0\n",
       "2      All wrong, back off dear, forward doubt. Stay ...    1.0\n",
       "3      I've shifted my focus to something else but I'...    1.0\n",
       "4      I'm restless and restless, it's been a month n...    1.0\n",
       "...                                                  ...    ...\n",
       "32511  \"Nothing beats the feeling of achieving someth...    0.0\n",
       "32512                Too much homework Feeling Stressed!    1.0\n",
       "32513        overworked in school,,  causes me  headache    1.0\n",
       "32514  How come my friends immediately get to know st...    0.0\n",
       "32515                                toady I wake at 5am    0.0\n",
       "\n",
       "[32516 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = ['dataset1.csv', 'dataset2.csv', 'dataset3.csv', 'dataset4.csv']#, 'dataset5.csv']\n",
    "\n",
    "data_df = get_data(files)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0.0    24406\n",
       "1.0     8105\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def even_out_dataframe(df):\n",
    "\n",
    "    counts = df['label'].value_counts()\n",
    "\n",
    "    if counts[0] > counts[1]:\n",
    "        desired_count = counts[1]\n",
    "        label = 0\n",
    "        non_label = 1\n",
    "    else:\n",
    "        desired_count = counts[0]\n",
    "        label = 1\n",
    "        non_label = 0\n",
    "\n",
    "    df_balanced = pd.concat([df[df['label'] == label].sample(desired_count), df[df['label'] == non_label]], ignore_index=True)\n",
    "    \n",
    "    return df_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0.0    8105\n",
      "1.0    8105\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "balanced_df = even_out_dataframe(data_df)\n",
    "print(balanced_df['label'].value_counts())\n",
    "\n",
    "data_df = balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Seeing the sunny weather this morning after it...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I think a girl looks cute when she's happy or ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@sarazarr GREAT! I didn't make out with you, b...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Julie Andrews\" Switzerland â™¥â '™ðŸ‡¨ðŸ‡.Swe...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wanted to tweet that i wa in victoria but sill...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16205</th>\n",
       "      <td>Depressed &amp; alone</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16206</th>\n",
       "      <td>I blame school for my anxiety</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16207</th>\n",
       "      <td>“If anyone could help me get through a problem.”</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16208</th>\n",
       "      <td>Too much homework Feeling Stressed!</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16209</th>\n",
       "      <td>overworked in school,,  causes me  headache</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16210 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0      Seeing the sunny weather this morning after it...    0.0\n",
       "1      I think a girl looks cute when she's happy or ...    0.0\n",
       "2      @sarazarr GREAT! I didn't make out with you, b...    0.0\n",
       "3      \"Julie Andrews\" Switzerland â™¥â '™ðŸ‡¨ðŸ‡.Swe...    0.0\n",
       "4      wanted to tweet that i wa in victoria but sill...    0.0\n",
       "...                                                  ...    ...\n",
       "16205                                  Depressed & alone    1.0\n",
       "16206                      I blame school for my anxiety    1.0\n",
       "16207   “If anyone could help me get through a problem.”    1.0\n",
       "16208                Too much homework Feeling Stressed!    1.0\n",
       "16209        overworked in school,,  causes me  headache    1.0\n",
       "\n",
       "[16210 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ming Xuan\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (856 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  4382\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "\n",
    "sentences = data_df['text']\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "    \n",
    "\n",
    "print('Max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = data_df['text']\n",
    "labels = data_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\Ming Xuan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2619: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  Seeing the sunny weather this morning after it rained for 4 days, mothers were excited to take out the laundry and take action, after that they smiled with relief looking at the clothesline in the hot sun and \"Let's sunbathe\" don't get vitamin D deficiency the cost of treating covid is \"expensive\" he said ðŸ ❤\n",
      "Token IDs: tensor([  101,  3773,  1996, 11559,  4633,  2023,  2851,  2044,  2009, 28270,\n",
      "         2005,  1018,  2420,  1010, 10756,  2020,  7568,  2000,  2202,  2041,\n",
      "         1996, 14533,  1998,  2202,  2895,  1010,  2044,  2008,  2027,  3281,\n",
      "         2007,  4335,  2559,  2012,  1996,  4253,  4179,  1999,  1996,  2980,\n",
      "         3103,  1998,  1000,  2292,  1005,  1055,  3103, 14479,  5369,  1000,\n",
      "         2123,  1005,  1056,  2131, 17663,  1040, 18888,  1996,  3465,  1997,\n",
      "        12318,  2522, 17258,   102])\n",
      "tensor([0., 0., 0.,  ..., 1., 1., 1.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', sentences[0])\n",
    "print('Token IDs:', input_ids[0])\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform labels to 1 hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        ...,\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.]])\n",
      "16210\n"
     ]
    }
   ],
   "source": [
    "num_classes = labels.max().item() + 1\n",
    "\n",
    "labels = labels.int()\n",
    "\n",
    "labels_saved = labels\n",
    "labels = torch.eye(num_classes)[labels]\n",
    "\n",
    "print(labels)\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12,968 training samples\n",
      "3,242 validation samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# Create a 80-20 train-validation split.\n",
    "TRAIN_SIZE = 0.8\n",
    "\n",
    "# Calculate the number of samples to include in each set.\n",
    "train_size = int(TRAIN_SIZE * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "bert_model = BertModel.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "bert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BERT_added_layers(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERT_added_layers, self).__init__()\n",
    "        \n",
    "        self.base_model = bert_model\n",
    "\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        self.tahn = nn.Tanh()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.linear1 = nn.Linear(768, 768)\n",
    "        self.linear2 = nn.Linear(768, 768)\n",
    "        self.linear4 = nn.Linear(768, 768)\n",
    "        self.linear3 = nn.Linear(768, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, input_ids, attn_mask):\n",
    "        output = self.base_model(input_ids, attention_mask=attn_mask)\n",
    "\n",
    "        output = self.linear1(output[0][:,0,:].view(-1,768))\n",
    "        output = self.tahn(output)\n",
    "        output = self.linear2(output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.linear4(output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.linear3(output)\n",
    "        output = self.sigmoid(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "model = BERT_added_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 207 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "base_model.embeddings.word_embeddings.weight            (30522, 768)\n",
      "base_model.embeddings.position_embeddings.weight          (512, 768)\n",
      "base_model.embeddings.token_type_embeddings.weight          (2, 768)\n",
      "base_model.embeddings.LayerNorm.weight                        (768,)\n",
      "base_model.embeddings.LayerNorm.bias                          (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "base_model.encoder.layer.0.attention.self.query.weight    (768, 768)\n",
      "base_model.encoder.layer.0.attention.self.query.bias          (768,)\n",
      "base_model.encoder.layer.0.attention.self.key.weight      (768, 768)\n",
      "base_model.encoder.layer.0.attention.self.key.bias            (768,)\n",
      "base_model.encoder.layer.0.attention.self.value.weight    (768, 768)\n",
      "base_model.encoder.layer.0.attention.self.value.bias          (768,)\n",
      "base_model.encoder.layer.0.attention.output.dense.weight   (768, 768)\n",
      "base_model.encoder.layer.0.attention.output.dense.bias        (768,)\n",
      "base_model.encoder.layer.0.attention.output.LayerNorm.weight       (768,)\n",
      "base_model.encoder.layer.0.attention.output.LayerNorm.bias       (768,)\n",
      "base_model.encoder.layer.0.intermediate.dense.weight     (3072, 768)\n",
      "base_model.encoder.layer.0.intermediate.dense.bias           (3072,)\n",
      "base_model.encoder.layer.0.output.dense.weight           (768, 3072)\n",
      "base_model.encoder.layer.0.output.dense.bias                  (768,)\n",
      "base_model.encoder.layer.0.output.LayerNorm.weight            (768,)\n",
      "base_model.encoder.layer.0.output.LayerNorm.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "linear1.weight                                            (768, 768)\n",
      "linear1.bias                                                  (768,)\n",
      "linear2.weight                                            (768, 768)\n",
      "linear2.bias                                                  (768,)\n",
      "linear4.weight                                            (768, 768)\n",
      "linear4.bias                                                  (768,)\n",
      "linear3.weight                                              (1, 768)\n",
      "linear3.bias                                                    (1,)\n"
     ]
    }
   ],
   "source": [
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-8:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "def f1_score_func(preds, labels):\n",
    "    # Takes in 2 tensors\n",
    "\n",
    "    preds, labels = preds.cpu().detach().numpy(), labels.cpu().detach().numpy()\n",
    "\n",
    "    f1 = f1_score(labels, preds, average = 'weighted')\n",
    "    p_score = precision_score(labels, preds)\n",
    "    r_score = recall_score(labels, preds)\n",
    "    \n",
    "    return f1, p_score, r_score\n",
    "\n",
    "def validate(model, valloader, val_dataset, device, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    batch_no = 0\n",
    "    running_f1 = 0\n",
    "    running_p_score = 0\n",
    "    running_r_score = 0\n",
    "    predicted_list, labels_list = [], []\n",
    "    \n",
    "    for batch in valloader:\n",
    "        batch_no += 1\n",
    "        tensor1, attn_mask,  label = batch[0], batch[1], batch[2]\n",
    "        tensor1, attn_mask, label = tensor1.to(device), attn_mask.to(device), label.to(device)\n",
    "        label = torch.argmax(label, dim=1).reshape(-1, 1)\n",
    "        with torch.no_grad():\n",
    "            output = model(tensor1, attn_mask)\n",
    "            preds = output>0.5\n",
    "            loss = criterion(output, label.float())\n",
    "            \n",
    "        val_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == label)\n",
    "\n",
    "        f1, p_score, r_score = f1_score_func(preds, label)\n",
    "        running_f1 += f1\n",
    "        running_p_score += p_score\n",
    "        running_r_score += r_score\n",
    "\n",
    "        predicted_list.extend(preds.cpu())\n",
    "        labels_list.extend(label.cpu())\n",
    "    \n",
    "    val_loss /= len(val_dataset)\n",
    "    val_acc = running_corrects.item()/len(val_dataset)\n",
    "    f1 = running_f1/batch_no\n",
    "    p_score = running_p_score/batch_no\n",
    "    r_score = running_r_score/batch_no\n",
    "\n",
    "    return val_loss, val_acc, f1 , p_score, r_score, predicted_list, labels_list\n",
    "\n",
    "def train(model, trainloader, train_dataset, optimizer, device, criterion, scheduler = None):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for batch in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        tensor1, attn_mask,  label = batch[0], batch[1], batch[2]\n",
    "        tensor1, attn_mask, label = tensor1.to(device), attn_mask.to(device), label.to(device)\n",
    "        output = model(tensor1, attn_mask)\n",
    "\n",
    "        preds = output>0.5\n",
    "        label = torch.argmax(label, dim=1).reshape(-1, 1)\n",
    "        \n",
    "        loss = criterion(output, label.float())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == label)\n",
    "\n",
    "    train_loss /= len(train_dataset)\n",
    "    train_acc = running_corrects.item()/len(train_dataset)\n",
    "\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1], \ttrain loss: 0.010068\tacc: 0.86151\n",
      "[1], \tval loss: 0.0080396\tacc: 0.8942\tf1: 0.89412\tp_score: 0.86368\tr_score: 0.93594\n",
      "[2], \ttrain loss: 0.0084768\tacc: 0.88788\n",
      "[2], \tval loss: 0.0075482\tacc: 0.91271\tf1: 0.91312\tp_score: 0.9519\tr_score: 0.87122\n",
      "[3], \ttrain loss: 0.0083733\tacc: 0.89158\n",
      "[3], \tval loss: 0.0072699\tacc: 0.90407\tf1: 0.90379\tp_score: 0.87159\tr_score: 0.94664\n",
      "[4], \ttrain loss: 0.0078719\tacc: 0.89813\n",
      "[4], \tval loss: 0.0069683\tacc: 0.91178\tf1: 0.91189\tp_score: 0.9725\tr_score: 0.85007\n",
      "[5], \ttrain loss: 0.0075396\tacc: 0.90022\n",
      "[5], \tval loss: 0.0067443\tacc: 0.92042\tf1: 0.92111\tp_score: 0.93272\tr_score: 0.90824\n",
      "[6], \ttrain loss: 0.0078118\tacc: 0.90238\n",
      "[6], \tval loss: 0.0079275\tacc: 0.90376\tf1: 0.90317\tp_score: 0.98016\tr_score: 0.82589\n",
      "[7], \ttrain loss: 0.0070089\tacc: 0.90932\n",
      "[7], \tval loss: 0.0062926\tacc: 0.92196\tf1: 0.92179\tp_score: 0.91805\tr_score: 0.92494\n",
      "[8], \ttrain loss: 0.0069498\tacc: 0.91194\n",
      "[8], \tval loss: 0.0060914\tacc: 0.92104\tf1: 0.9216\tp_score: 0.90988\tr_score: 0.93582\n",
      "[9], \ttrain loss: 0.0071619\tacc: 0.90947\n",
      "[9], \tval loss: 0.0058434\tacc: 0.92875\tf1: 0.92849\tp_score: 0.94483\tr_score: 0.90811\n",
      "[10], \ttrain loss: 0.0069759\tacc: 0.90947\n",
      "[10], \tval loss: 0.0061028\tacc: 0.91949\tf1: 0.91923\tp_score: 0.91329\tr_score: 0.92554\n",
      "[11], \ttrain loss: 0.0075322\tacc: 0.90947\n",
      "[11], \tval loss: 0.005504\tacc: 0.93368\tf1: 0.93286\tp_score: 0.95348\tr_score: 0.91152\n",
      "[12], \ttrain loss: 0.0066586\tacc: 0.91602\n",
      "[12], \tval loss: 0.0098887\tacc: 0.86613\tf1: 0.86212\tp_score: 0.79974\tr_score: 0.97481\n",
      "[13], \ttrain loss: 0.0062531\tacc: 0.92019\n",
      "[13], \tval loss: 0.0063803\tacc: 0.91826\tf1: 0.91863\tp_score: 0.89519\tr_score: 0.94906\n",
      "[14], \ttrain loss: 0.0067643\tacc: 0.91556\n",
      "[14], \tval loss: 0.0077589\tacc: 0.91271\tf1: 0.91277\tp_score: 0.99036\tr_score: 0.83727\n",
      "Done! Early stopped at 13\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from utils.early_stopper import EarlyStopper\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "lr = 0.005\n",
    "optimizer = torch.optim.AdamW(params= model.parameters(), lr = lr)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "epochs = 30\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "patience = 3\n",
    "early_stopper = EarlyStopper(patience=patience, min_delta=0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "num_epoch = 50\n",
    "best_epoch = 0\n",
    "best_val_acc = 0\n",
    "\n",
    "history = []\n",
    "accuracy = []\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_dataloader, train_dataset, optimizer, device, criterion)\n",
    "    val_loss, val_acc, f1 , p_score, r_score, predicted_list, labels_list  = validate(model, validation_dataloader, val_dataset, device, criterion)\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_epoch = epoch\n",
    "\n",
    "        stored_predicted_list, stored_labels_list = predicted_list, labels_list\n",
    "\n",
    "        # save_path = os.getcwd() +'\\\\models\\\\{}_best.pt'.format(name)\n",
    "        # torch.save(model.state_dict(), save_path)\n",
    "\n",
    "    print('[{}], \\ttrain loss: {:.5}\\tacc: {:.5}'.format(epoch+1, train_loss, train_acc))\n",
    "    print('[{}], \\tval loss: {:.5}\\tacc: {:.5}\\tf1: {:.5}\\tp_score: {:.5}\\tr_score: {:.5}'.format(epoch+1, val_loss, val_acc, f1 , p_score, r_score))\n",
    "\n",
    "    if early_stopper.early_stop(val_loss):\n",
    "        print(\"Done! Early stopped at {}\".format(epoch))\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "from sklearn import metrics\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(stored_labels_list, stored_predicted_list)\n",
    "\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "\n",
    "\n",
    "cm_display.plot()\n",
    "plt.title(\"BERT 1-extra layer Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ming Xuan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2619: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "BERT_added_layers.forward() missing 1 required positional argument: 'attn_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 38\u001b[0m\n\u001b[0;32m     34\u001b[0m model\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(input_ids)):\n\u001b[1;32m---> 38\u001b[0m     predicted \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m(predicted[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     40\u001b[0m     predicted \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(predicted[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[1;32mc:\\Users\\Ming Xuan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ming Xuan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: BERT_added_layers.forward() missing 1 required positional argument: 'attn_mask'"
     ]
    }
   ],
   "source": [
    "A = 'All I feel is anxiety'\n",
    "B = 'naruto is a great anime'\n",
    "C = 'I dont feel anything anymore'\n",
    "D = 'Lets have a picnic today!'\n",
    "E = 'I am wondering why there is so much talk about depression these days'\n",
    "F = 'I have never been happier'\n",
    "\n",
    "sentences = [A,B,C,D,E,F]\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "model.cpu()\n",
    "\n",
    "for x in range(len(input_ids)):\n",
    "\n",
    "    predicted = model(input_ids[x])\n",
    "    print(predicted[0][0])\n",
    "    predicted = np.argmax(predicted[0][0].detach().numpy())\n",
    "\n",
    "\n",
    "    print(\"For ({}) : {}\".format(sentences[x], predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
